{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='UTF8') as f:\n",
    "        str_text = f.read()\n",
    "        \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMMITTEE: General Assembly\\nQUESTION OF: Measures to Significantly Reduce all Forms of Violence and Related Death Rates EverywhereMAIN MAIN SUBMITTER:Brazil\\nCO-SUBMITTER: Argentina, China Czech Republic\\n\\n\\nTHE UNITED NATIONS GENERAL ASSEMBLY,\\n\\n\\nRecalling Sustainable Development Goal 16 - peace, justice, and strong institution\\n\\n\\nDisturbed that one out of four Chinese housewives is beaten by their husbands,s\\n\\n\\nTaking into consideration, gun/domestic violence has been a serious crime that led to an approximate of 60,000 deaths in a single year,\\n\\n\\nEmphasizing the fact 73,505 injuries and 33,636 Deaths related to firearms,\\n\\n\\nDeeply concerned by the fact that numerous countries around the world are still not taking an immediate and comprehensive resolution to the huge issue of gun violence\\n\\n\\n1.  Calls upon all member states to restrict individuals from using domestic violence to hurt, abuse or kill people because of reasons by:\\n1. Increase the time of imprisonment globally when domestic violence happens so that,\\n2. Do not provide tolerance to people in court unless the matter is small and can be excused with a small punishment\\n3. Actively promoting governmental organizations or NGO where women can easily reach out help whenever domestic violence occurs;\\n\\n\\n2. Encourages all governments to prohibit the use of gun usage by enforcing laws and educating the young about the mass destruction caused by guns by:\\n   1. No firearms allowed except during political wars or other situations officially accepted by the government\\n   2. Gradually decrease the production of guns from companies\\n   3. Sending professionals to schools to educate the young generation;\\n\\n\\nGUYS USE THIS WEBSITE FOR PREAMBULATORY CLAUSE+OPERATIVE CLAUSE HEADINGS\\n->>>>> LINK\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1 min Speech\\n\\n\\nThis resolution should be passed by the community because of the reasons that this delegate is going to talk about. First, the delegates’ resolution supports the ban of firearms, which is helpful for the international community, because firearms are creating mass destructions along with numerous casualties of innocent people. These firearms are created by companies that mass produce guns. These solutions all base on the vital responsibility of countries to educate the young generation about the dangers and harms of gun violence. Gun violence may also intentionally be used in different focuses, and this may include domestic violence. Domestic violence also is a current issue, harming citizens especially women by assaulting and raping. Our resolution also further effectively block domestic violence by giving harsher punishments and elongating the imprisonment period or providing tolerance to domestic violence criminals, but most significantly promoting organizations where women can eagerly reach out help whenever domestic violence occurs. \\n\\n\\nRebut 2 the POI: No, since the young generation will learn abt guns at a young age, they are capable of learning about guns more effectively and will be able to learn abt harming parts and \\nPOI2: Harsher Measures on guns, thats what we said like harsher rules on guns so no illegal, we will take care of that with \\nPOI3: HARSHER RULES, so no illegal guns will be sold online or offline since countries will block them/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('data/doc_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('data/doc_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\r\\n \\n\\n \\n\\n\\n!\"-#$%&()--.*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('data/doc_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['committee',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'question',\n",
       " 'of',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'significantly',\n",
       " 'reduce',\n",
       " 'all',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'related',\n",
       " 'death',\n",
       " 'rates',\n",
       " 'everywheremain',\n",
       " 'main',\n",
       " 'submitter',\n",
       " 'brazil',\n",
       " 'co',\n",
       " 'submitter',\n",
       " 'argentina',\n",
       " 'china',\n",
       " 'czech',\n",
       " 'republic',\n",
       " 'the',\n",
       " 'united',\n",
       " 'nations',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'recalling',\n",
       " 'sustainable',\n",
       " 'development',\n",
       " 'goal',\n",
       " '16',\n",
       " 'peace',\n",
       " 'justice',\n",
       " 'and',\n",
       " 'strong',\n",
       " 'institution',\n",
       " 'disturbed',\n",
       " 'that',\n",
       " 'one',\n",
       " 'out',\n",
       " 'of',\n",
       " 'four',\n",
       " 'chinese',\n",
       " 'housewives',\n",
       " 'is',\n",
       " 'beaten',\n",
       " 'by',\n",
       " 'their',\n",
       " 'husbands',\n",
       " 's',\n",
       " 'taking',\n",
       " 'into',\n",
       " 'consideration',\n",
       " 'gun',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'has',\n",
       " 'been',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'crime',\n",
       " 'that',\n",
       " 'led',\n",
       " 'to',\n",
       " 'an',\n",
       " 'approximate',\n",
       " 'of',\n",
       " '60,000',\n",
       " 'deaths',\n",
       " 'in',\n",
       " 'a',\n",
       " 'single',\n",
       " 'year',\n",
       " 'emphasizing',\n",
       " 'the',\n",
       " 'fact',\n",
       " '73,505',\n",
       " 'injuries',\n",
       " 'and',\n",
       " '33,636',\n",
       " 'deaths',\n",
       " 'related',\n",
       " 'to',\n",
       " 'firearms',\n",
       " 'deeply',\n",
       " 'concerned',\n",
       " 'by',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'numerous',\n",
       " 'countries',\n",
       " 'around',\n",
       " 'the',\n",
       " 'world',\n",
       " 'are',\n",
       " 'still',\n",
       " 'not',\n",
       " 'taking',\n",
       " 'an',\n",
       " 'immediate',\n",
       " 'and',\n",
       " 'comprehensive',\n",
       " 'resolution',\n",
       " 'to',\n",
       " 'the',\n",
       " 'huge',\n",
       " 'issue',\n",
       " 'of',\n",
       " 'gun',\n",
       " 'violence',\n",
       " '1',\n",
       " 'calls',\n",
       " 'upon',\n",
       " 'all',\n",
       " 'member',\n",
       " 'states',\n",
       " 'to',\n",
       " 'restrict',\n",
       " 'individuals',\n",
       " 'from',\n",
       " 'using',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'to',\n",
       " 'hurt',\n",
       " 'abuse',\n",
       " 'or',\n",
       " 'kill',\n",
       " 'people',\n",
       " 'because',\n",
       " 'of',\n",
       " 'reasons',\n",
       " 'by',\n",
       " '1',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'imprisonment',\n",
       " 'globally',\n",
       " 'when',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'happens',\n",
       " 'so',\n",
       " 'that',\n",
       " '2',\n",
       " 'do',\n",
       " 'not',\n",
       " 'provide',\n",
       " 'tolerance',\n",
       " 'to',\n",
       " 'people',\n",
       " 'in',\n",
       " 'court',\n",
       " 'unless',\n",
       " 'the',\n",
       " 'matter',\n",
       " 'is',\n",
       " 'small',\n",
       " 'and',\n",
       " 'can',\n",
       " 'be',\n",
       " 'excused',\n",
       " 'with',\n",
       " 'a',\n",
       " 'small',\n",
       " 'punishment',\n",
       " '3',\n",
       " 'actively',\n",
       " 'promoting',\n",
       " 'governmental',\n",
       " 'organizations',\n",
       " 'or',\n",
       " 'ngo',\n",
       " 'where',\n",
       " 'women',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'reach',\n",
       " 'out',\n",
       " 'help',\n",
       " 'whenever',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'occurs',\n",
       " '2',\n",
       " 'encourages',\n",
       " 'all',\n",
       " 'governments',\n",
       " 'to',\n",
       " 'prohibit',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'gun',\n",
       " 'usage',\n",
       " 'by',\n",
       " 'enforcing',\n",
       " 'laws',\n",
       " 'and',\n",
       " 'educating',\n",
       " 'the',\n",
       " 'young',\n",
       " 'about',\n",
       " 'the',\n",
       " 'mass',\n",
       " 'destruction',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'guns',\n",
       " 'by',\n",
       " '\\n   ',\n",
       " '1',\n",
       " 'no',\n",
       " 'firearms',\n",
       " 'allowed',\n",
       " 'except',\n",
       " 'during',\n",
       " 'political',\n",
       " 'wars',\n",
       " 'or',\n",
       " 'other',\n",
       " 'situations',\n",
       " 'officially',\n",
       " 'accepted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'government',\n",
       " '\\n   ',\n",
       " '2',\n",
       " 'gradually',\n",
       " 'decrease',\n",
       " 'the',\n",
       " 'production',\n",
       " 'of',\n",
       " 'guns',\n",
       " 'from',\n",
       " 'companies',\n",
       " '\\n   ',\n",
       " '3',\n",
       " 'sending',\n",
       " 'professionals',\n",
       " 'to',\n",
       " 'schools',\n",
       " 'to',\n",
       " 'educate',\n",
       " 'the',\n",
       " 'young',\n",
       " 'generation',\n",
       " 'guys',\n",
       " 'use',\n",
       " 'this',\n",
       " 'website',\n",
       " 'for',\n",
       " 'preambulatory',\n",
       " 'clause+operative',\n",
       " 'clause',\n",
       " 'headings',\n",
       " 'link',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '1',\n",
       " 'min',\n",
       " 'speech',\n",
       " 'this',\n",
       " 'resolution',\n",
       " 'should',\n",
       " 'be',\n",
       " 'passed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'community',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reasons',\n",
       " 'that',\n",
       " 'this',\n",
       " 'delegate',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'first',\n",
       " 'the',\n",
       " 'delegates',\n",
       " '’',\n",
       " 'resolution',\n",
       " 'supports',\n",
       " 'the',\n",
       " 'ban',\n",
       " 'of',\n",
       " 'firearms',\n",
       " 'which',\n",
       " 'is',\n",
       " 'helpful',\n",
       " 'for',\n",
       " 'the',\n",
       " 'international',\n",
       " 'community',\n",
       " 'because',\n",
       " 'firearms',\n",
       " 'are',\n",
       " 'creating',\n",
       " 'mass',\n",
       " 'destructions',\n",
       " 'along',\n",
       " 'with',\n",
       " 'numerous',\n",
       " 'casualties',\n",
       " 'of',\n",
       " 'innocent',\n",
       " 'people',\n",
       " 'these',\n",
       " 'firearms',\n",
       " 'are',\n",
       " 'created',\n",
       " 'by',\n",
       " 'companies',\n",
       " 'that',\n",
       " 'mass',\n",
       " 'produce',\n",
       " 'guns',\n",
       " 'these',\n",
       " 'solutions',\n",
       " 'all',\n",
       " 'base',\n",
       " 'on',\n",
       " 'the',\n",
       " 'vital',\n",
       " 'responsibility',\n",
       " 'of',\n",
       " 'countries',\n",
       " 'to',\n",
       " 'educate',\n",
       " 'the',\n",
       " 'young',\n",
       " 'generation',\n",
       " 'about',\n",
       " 'the',\n",
       " 'dangers',\n",
       " 'and',\n",
       " 'harms',\n",
       " 'of',\n",
       " 'gun',\n",
       " 'violence',\n",
       " 'gun',\n",
       " 'violence',\n",
       " 'may',\n",
       " 'also',\n",
       " 'intentionally',\n",
       " 'be',\n",
       " 'used',\n",
       " 'in',\n",
       " 'different',\n",
       " 'focuses',\n",
       " 'and',\n",
       " 'this',\n",
       " 'may',\n",
       " 'include',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'also',\n",
       " 'is',\n",
       " 'a',\n",
       " 'current',\n",
       " 'issue',\n",
       " 'harming',\n",
       " 'citizens',\n",
       " 'especially',\n",
       " 'women',\n",
       " 'by',\n",
       " 'assaulting',\n",
       " 'and',\n",
       " 'raping',\n",
       " 'our',\n",
       " 'resolution',\n",
       " 'also',\n",
       " 'further',\n",
       " 'effectively',\n",
       " 'block',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'by',\n",
       " 'giving',\n",
       " 'harsher',\n",
       " 'punishments',\n",
       " 'and',\n",
       " 'elongating',\n",
       " 'the',\n",
       " 'imprisonment',\n",
       " 'period',\n",
       " 'or',\n",
       " 'providing',\n",
       " 'tolerance',\n",
       " 'to',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'criminals',\n",
       " 'but',\n",
       " 'most',\n",
       " 'significantly',\n",
       " 'promoting',\n",
       " 'organizations',\n",
       " 'where',\n",
       " 'women',\n",
       " 'can',\n",
       " 'eagerly',\n",
       " 'reach',\n",
       " 'out',\n",
       " 'help',\n",
       " 'whenever',\n",
       " 'domestic',\n",
       " 'violence',\n",
       " 'occurs',\n",
       " 'rebut',\n",
       " '2',\n",
       " 'the',\n",
       " 'poi',\n",
       " 'no',\n",
       " 'since',\n",
       " 'the',\n",
       " 'young',\n",
       " 'generation',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'abt',\n",
       " 'guns',\n",
       " 'at',\n",
       " 'a',\n",
       " 'young',\n",
       " 'age',\n",
       " 'they',\n",
       " 'are',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'guns',\n",
       " 'more',\n",
       " 'effectively',\n",
       " 'and',\n",
       " 'will',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'abt',\n",
       " 'harming',\n",
       " 'parts',\n",
       " 'and',\n",
       " 'poi2',\n",
       " 'harsher',\n",
       " 'measures',\n",
       " 'on',\n",
       " 'guns',\n",
       " 'that',\n",
       " 's',\n",
       " 'what',\n",
       " 'we',\n",
       " 'said',\n",
       " 'like',\n",
       " 'harsher',\n",
       " 'rules',\n",
       " 'on',\n",
       " 'guns',\n",
       " 'so',\n",
       " 'no',\n",
       " 'illegal',\n",
       " 'we',\n",
       " 'will',\n",
       " 'take',\n",
       " 'care',\n",
       " 'of',\n",
       " 'that',\n",
       " 'with',\n",
       " 'poi3',\n",
       " 'harsher',\n",
       " 'rules',\n",
       " 'so',\n",
       " 'no',\n",
       " 'illegal',\n",
       " 'guns',\n",
       " 'will',\n",
       " 'be',\n",
       " 'sold',\n",
       " 'online',\n",
       " 'or',\n",
       " 'offline',\n",
       " 'since',\n",
       " 'countries',\n",
       " 'will',\n",
       " 'block',\n",
       " 'them/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#25 words ---> networkpredict #26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "        \n",
    "    text_sequences.append(seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['committee',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'question',\n",
       " 'of',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'significantly',\n",
       " 'reduce',\n",
       " 'all',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'related',\n",
       " 'death',\n",
       " 'rates',\n",
       " 'everywheremain',\n",
       " 'main',\n",
       " 'submitter',\n",
       " 'brazil',\n",
       " 'co',\n",
       " 'submitter',\n",
       " 'argentina',\n",
       " 'china',\n",
       " 'czech']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0] #check 첫번재 list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['general',\n",
       " 'assembly',\n",
       " 'question',\n",
       " 'of',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'significantly',\n",
       " 'reduce',\n",
       " 'all',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'related',\n",
       " 'death',\n",
       " 'rates',\n",
       " 'everywheremain',\n",
       " 'main',\n",
       " 'submitter',\n",
       " 'brazil',\n",
       " 'co',\n",
       " 'submitter',\n",
       " 'argentina',\n",
       " 'china',\n",
       " 'czech',\n",
       " 'republic']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[1] #next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'committee general assembly question of measures to significantly reduce all forms of violence and related death rates everywheremain main submitter brazil co submitter argentina china czech'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general assembly question of measures to significantly reduce all forms of violence and related death rates everywheremain main submitter brazil co submitter argentina china czech republic'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[267,\n",
       " 82,\n",
       " 81,\n",
       " 266,\n",
       " 2,\n",
       " 79,\n",
       " 3,\n",
       " 78,\n",
       " 262,\n",
       " 25,\n",
       " 261,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 75,\n",
       " 260,\n",
       " 258,\n",
       " 257,\n",
       " 256,\n",
       " 74,\n",
       " 254,\n",
       " 252,\n",
       " 74,\n",
       " 251,\n",
       " 250,\n",
       " 84]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0] #serises of numbers..  267, 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82,\n",
       " 81,\n",
       " 266,\n",
       " 2,\n",
       " 79,\n",
       " 3,\n",
       " 78,\n",
       " 262,\n",
       " 25,\n",
       " 261,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 75,\n",
       " 260,\n",
       " 258,\n",
       " 257,\n",
       " 256,\n",
       " 74,\n",
       " 254,\n",
       " 252,\n",
       " 74,\n",
       " 251,\n",
       " 250,\n",
       " 84,\n",
       " 85]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1] #serises of numbers.. 82, 81 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'of',\n",
       " 3: 'to',\n",
       " 4: 'violence',\n",
       " 5: 'and',\n",
       " 6: 'by',\n",
       " 7: 'domestic',\n",
       " 8: 'that',\n",
       " 9: 'guns',\n",
       " 10: 'is',\n",
       " 11: 'gun',\n",
       " 12: 'a',\n",
       " 13: 'firearms',\n",
       " 14: 'young',\n",
       " 15: 'be',\n",
       " 16: 'or',\n",
       " 17: 'are',\n",
       " 18: 'resolution',\n",
       " 19: '1',\n",
       " 20: '2',\n",
       " 21: 'about',\n",
       " 22: 'this',\n",
       " 23: 'harsher',\n",
       " 24: 'no',\n",
       " 25: 'all',\n",
       " 26: 'will',\n",
       " 27: 'out',\n",
       " 28: 'in',\n",
       " 29: 'people',\n",
       " 30: 'because',\n",
       " 31: 'can',\n",
       " 32: 'women',\n",
       " 33: 'mass',\n",
       " 34: '\\n   ',\n",
       " 35: 'generation',\n",
       " 36: 'on',\n",
       " 37: 'also',\n",
       " 38: 'with',\n",
       " 39: 'so',\n",
       " 40: 'countries',\n",
       " 41: 's',\n",
       " 42: 'taking',\n",
       " 43: 'an',\n",
       " 44: 'deaths',\n",
       " 45: 'fact',\n",
       " 46: 'numerous',\n",
       " 47: 'not',\n",
       " 48: 'issue',\n",
       " 49: 'from',\n",
       " 50: 'reasons',\n",
       " 51: 'imprisonment',\n",
       " 52: 'tolerance',\n",
       " 53: 'small',\n",
       " 54: '3',\n",
       " 55: 'promoting',\n",
       " 56: 'organizations',\n",
       " 57: 'where',\n",
       " 58: 'reach',\n",
       " 59: 'help',\n",
       " 60: 'whenever',\n",
       " 61: 'occurs',\n",
       " 62: 'use',\n",
       " 63: 'companies',\n",
       " 64: 'educate',\n",
       " 65: 'for',\n",
       " 66: 'community',\n",
       " 67: 'these',\n",
       " 68: 'may',\n",
       " 69: 'harming',\n",
       " 70: 'effectively',\n",
       " 71: 'learn',\n",
       " 72: 'abt',\n",
       " 73: 'we',\n",
       " 74: 'submitter',\n",
       " 75: 'related',\n",
       " 76: 'rules',\n",
       " 77: 'illegal',\n",
       " 78: 'significantly',\n",
       " 79: 'measures',\n",
       " 80: 'since',\n",
       " 81: 'assembly',\n",
       " 82: 'general',\n",
       " 83: 'block',\n",
       " 84: 'czech',\n",
       " 85: 'republic',\n",
       " 86: 'united',\n",
       " 87: 'nations',\n",
       " 88: 'recalling',\n",
       " 89: 'sustainable',\n",
       " 90: 'development',\n",
       " 91: 'goal',\n",
       " 92: '16',\n",
       " 93: 'peace',\n",
       " 94: 'justice',\n",
       " 95: 'strong',\n",
       " 96: 'institution',\n",
       " 97: 'disturbed',\n",
       " 98: 'one',\n",
       " 99: 'four',\n",
       " 100: 'chinese',\n",
       " 101: 'housewives',\n",
       " 102: 'beaten',\n",
       " 103: 'their',\n",
       " 104: 'husbands',\n",
       " 105: 'into',\n",
       " 106: 'consideration',\n",
       " 107: 'has',\n",
       " 108: 'been',\n",
       " 109: 'serious',\n",
       " 110: 'crime',\n",
       " 111: 'led',\n",
       " 112: 'approximate',\n",
       " 113: '60,000',\n",
       " 114: 'single',\n",
       " 115: 'year',\n",
       " 116: 'emphasizing',\n",
       " 117: '73,505',\n",
       " 118: 'injuries',\n",
       " 119: '33,636',\n",
       " 120: 'deeply',\n",
       " 121: 'concerned',\n",
       " 122: 'around',\n",
       " 123: 'world',\n",
       " 124: 'still',\n",
       " 125: 'immediate',\n",
       " 126: 'comprehensive',\n",
       " 127: 'huge',\n",
       " 128: 'calls',\n",
       " 129: 'upon',\n",
       " 130: 'member',\n",
       " 131: 'states',\n",
       " 132: 'restrict',\n",
       " 133: 'individuals',\n",
       " 134: 'using',\n",
       " 135: 'hurt',\n",
       " 136: 'abuse',\n",
       " 137: 'kill',\n",
       " 138: 'increase',\n",
       " 139: 'time',\n",
       " 140: 'globally',\n",
       " 141: 'when',\n",
       " 142: 'happens',\n",
       " 143: 'do',\n",
       " 144: 'provide',\n",
       " 145: 'court',\n",
       " 146: 'unless',\n",
       " 147: 'matter',\n",
       " 148: 'excused',\n",
       " 149: 'punishment',\n",
       " 150: 'actively',\n",
       " 151: 'governmental',\n",
       " 152: 'ngo',\n",
       " 153: 'easily',\n",
       " 154: 'encourages',\n",
       " 155: 'governments',\n",
       " 156: 'prohibit',\n",
       " 157: 'usage',\n",
       " 158: 'enforcing',\n",
       " 159: 'laws',\n",
       " 160: 'educating',\n",
       " 161: 'destruction',\n",
       " 162: 'caused',\n",
       " 163: 'allowed',\n",
       " 164: 'except',\n",
       " 165: 'during',\n",
       " 166: 'political',\n",
       " 167: 'wars',\n",
       " 168: 'other',\n",
       " 169: 'situations',\n",
       " 170: 'officially',\n",
       " 171: 'accepted',\n",
       " 172: 'government',\n",
       " 173: 'gradually',\n",
       " 174: 'decrease',\n",
       " 175: 'production',\n",
       " 176: 'sending',\n",
       " 177: 'professionals',\n",
       " 178: 'schools',\n",
       " 179: 'guys',\n",
       " 180: 'website',\n",
       " 181: 'preambulatory',\n",
       " 182: 'clause+operative',\n",
       " 183: 'clause',\n",
       " 184: 'headings',\n",
       " 185: 'link',\n",
       " 186: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 187: 'min',\n",
       " 188: 'speech',\n",
       " 189: 'should',\n",
       " 190: 'passed',\n",
       " 191: 'delegate',\n",
       " 192: 'going',\n",
       " 193: 'talk',\n",
       " 194: 'first',\n",
       " 195: 'delegates',\n",
       " 196: '’',\n",
       " 197: 'supports',\n",
       " 198: 'ban',\n",
       " 199: 'which',\n",
       " 200: 'helpful',\n",
       " 201: 'international',\n",
       " 202: 'creating',\n",
       " 203: 'destructions',\n",
       " 204: 'along',\n",
       " 205: 'casualties',\n",
       " 206: 'innocent',\n",
       " 207: 'created',\n",
       " 208: 'produce',\n",
       " 209: 'solutions',\n",
       " 210: 'base',\n",
       " 211: 'vital',\n",
       " 212: 'responsibility',\n",
       " 213: 'dangers',\n",
       " 214: 'harms',\n",
       " 215: 'intentionally',\n",
       " 216: 'used',\n",
       " 217: 'different',\n",
       " 218: 'focuses',\n",
       " 219: 'include',\n",
       " 220: 'current',\n",
       " 221: 'citizens',\n",
       " 222: 'especially',\n",
       " 223: 'assaulting',\n",
       " 224: 'raping',\n",
       " 225: 'our',\n",
       " 226: 'further',\n",
       " 227: 'giving',\n",
       " 228: 'punishments',\n",
       " 229: 'elongating',\n",
       " 230: 'period',\n",
       " 231: 'providing',\n",
       " 232: 'criminals',\n",
       " 233: 'but',\n",
       " 234: 'most',\n",
       " 235: 'eagerly',\n",
       " 236: 'rebut',\n",
       " 237: 'poi',\n",
       " 238: 'at',\n",
       " 239: 'age',\n",
       " 240: 'they',\n",
       " 241: 'capable',\n",
       " 242: 'learning',\n",
       " 243: 'more',\n",
       " 244: 'able',\n",
       " 245: 'parts',\n",
       " 246: 'poi2',\n",
       " 247: 'what',\n",
       " 248: 'said',\n",
       " 249: 'like',\n",
       " 250: 'china',\n",
       " 251: 'argentina',\n",
       " 252: 'co',\n",
       " 253: 'take',\n",
       " 254: 'brazil',\n",
       " 255: 'care',\n",
       " 256: 'main',\n",
       " 257: 'everywheremain',\n",
       " 258: 'rates',\n",
       " 259: 'poi3',\n",
       " 260: 'death',\n",
       " 261: 'forms',\n",
       " 262: 'reduce',\n",
       " 263: 'sold',\n",
       " 264: 'online',\n",
       " 265: 'offline',\n",
       " 266: 'question',\n",
       " 267: 'committee'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.index_word #check index to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 : committee\n",
      "82 : general\n",
      "81 : assembly\n",
      "266 : question\n",
      "2 : of\n",
      "79 : measures\n",
      "3 : to\n",
      "78 : significantly\n",
      "262 : reduce\n",
      "25 : all\n",
      "261 : forms\n",
      "2 : of\n",
      "4 : violence\n",
      "5 : and\n",
      "75 : related\n",
      "260 : death\n",
      "258 : rates\n",
      "257 : everywheremain\n",
      "256 : main\n",
      "74 : submitter\n",
      "254 : brazil\n",
      "252 : co\n",
      "74 : submitter\n",
      "251 : argentina\n",
      "250 : china\n",
      "84 : czech\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i} : {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('committee', 1),\n",
       "             ('general', 28),\n",
       "             ('assembly', 29),\n",
       "             ('question', 4),\n",
       "             ('of', 375),\n",
       "             ('measures', 32),\n",
       "             ('to', 345),\n",
       "             ('significantly', 34),\n",
       "             ('reduce', 9),\n",
       "             ('all', 88),\n",
       "             ('forms', 11),\n",
       "             ('violence', 325),\n",
       "             ('and', 300),\n",
       "             ('related', 41),\n",
       "             ('death', 16),\n",
       "             ('rates', 17),\n",
       "             ('everywheremain', 18),\n",
       "             ('main', 19),\n",
       "             ('submitter', 43),\n",
       "             ('brazil', 21),\n",
       "             ('co', 22),\n",
       "             ('argentina', 24),\n",
       "             ('china', 25),\n",
       "             ('czech', 26),\n",
       "             ('republic', 26),\n",
       "             ('the', 624),\n",
       "             ('united', 26),\n",
       "             ('nations', 26),\n",
       "             ('recalling', 26),\n",
       "             ('sustainable', 26),\n",
       "             ('development', 26),\n",
       "             ('goal', 26),\n",
       "             ('16', 26),\n",
       "             ('peace', 26),\n",
       "             ('justice', 26),\n",
       "             ('strong', 26),\n",
       "             ('institution', 26),\n",
       "             ('disturbed', 26),\n",
       "             ('that', 201),\n",
       "             ('one', 26),\n",
       "             ('out', 78),\n",
       "             ('four', 26),\n",
       "             ('chinese', 26),\n",
       "             ('housewives', 26),\n",
       "             ('is', 130),\n",
       "             ('beaten', 26),\n",
       "             ('by', 286),\n",
       "             ('their', 26),\n",
       "             ('husbands', 26),\n",
       "             ('s', 52),\n",
       "             ('taking', 52),\n",
       "             ('into', 26),\n",
       "             ('consideration', 26),\n",
       "             ('gun', 130),\n",
       "             ('domestic', 234),\n",
       "             ('has', 26),\n",
       "             ('been', 26),\n",
       "             ('a', 130),\n",
       "             ('serious', 26),\n",
       "             ('crime', 26),\n",
       "             ('led', 26),\n",
       "             ('an', 52),\n",
       "             ('approximate', 26),\n",
       "             ('60,000', 26),\n",
       "             ('deaths', 52),\n",
       "             ('in', 78),\n",
       "             ('single', 26),\n",
       "             ('year', 26),\n",
       "             ('emphasizing', 26),\n",
       "             ('fact', 52),\n",
       "             ('73,505', 26),\n",
       "             ('injuries', 26),\n",
       "             ('33,636', 26),\n",
       "             ('firearms', 130),\n",
       "             ('deeply', 26),\n",
       "             ('concerned', 26),\n",
       "             ('numerous', 52),\n",
       "             ('countries', 55),\n",
       "             ('around', 26),\n",
       "             ('world', 26),\n",
       "             ('are', 104),\n",
       "             ('still', 26),\n",
       "             ('not', 52),\n",
       "             ('immediate', 26),\n",
       "             ('comprehensive', 26),\n",
       "             ('resolution', 104),\n",
       "             ('huge', 26),\n",
       "             ('issue', 52),\n",
       "             ('1', 104),\n",
       "             ('calls', 26),\n",
       "             ('upon', 26),\n",
       "             ('member', 26),\n",
       "             ('states', 26),\n",
       "             ('restrict', 26),\n",
       "             ('individuals', 26),\n",
       "             ('from', 52),\n",
       "             ('using', 26),\n",
       "             ('hurt', 26),\n",
       "             ('abuse', 26),\n",
       "             ('or', 110),\n",
       "             ('kill', 26),\n",
       "             ('people', 78),\n",
       "             ('because', 78),\n",
       "             ('reasons', 52),\n",
       "             ('increase', 26),\n",
       "             ('time', 26),\n",
       "             ('imprisonment', 52),\n",
       "             ('globally', 26),\n",
       "             ('when', 26),\n",
       "             ('happens', 26),\n",
       "             ('so', 66),\n",
       "             ('2', 104),\n",
       "             ('do', 26),\n",
       "             ('provide', 26),\n",
       "             ('tolerance', 52),\n",
       "             ('court', 26),\n",
       "             ('unless', 26),\n",
       "             ('matter', 26),\n",
       "             ('small', 52),\n",
       "             ('can', 78),\n",
       "             ('be', 113),\n",
       "             ('excused', 26),\n",
       "             ('with', 70),\n",
       "             ('punishment', 26),\n",
       "             ('3', 52),\n",
       "             ('actively', 26),\n",
       "             ('promoting', 52),\n",
       "             ('governmental', 26),\n",
       "             ('organizations', 52),\n",
       "             ('ngo', 26),\n",
       "             ('where', 52),\n",
       "             ('women', 78),\n",
       "             ('easily', 26),\n",
       "             ('reach', 52),\n",
       "             ('help', 52),\n",
       "             ('whenever', 52),\n",
       "             ('occurs', 52),\n",
       "             ('encourages', 26),\n",
       "             ('governments', 26),\n",
       "             ('prohibit', 26),\n",
       "             ('use', 52),\n",
       "             ('usage', 26),\n",
       "             ('enforcing', 26),\n",
       "             ('laws', 26),\n",
       "             ('educating', 26),\n",
       "             ('young', 130),\n",
       "             ('about', 104),\n",
       "             ('mass', 78),\n",
       "             ('destruction', 26),\n",
       "             ('caused', 26),\n",
       "             ('guns', 193),\n",
       "             ('\\n   ', 78),\n",
       "             ('no', 91),\n",
       "             ('allowed', 26),\n",
       "             ('except', 26),\n",
       "             ('during', 26),\n",
       "             ('political', 26),\n",
       "             ('wars', 26),\n",
       "             ('other', 26),\n",
       "             ('situations', 26),\n",
       "             ('officially', 26),\n",
       "             ('accepted', 26),\n",
       "             ('government', 26),\n",
       "             ('gradually', 26),\n",
       "             ('decrease', 26),\n",
       "             ('production', 26),\n",
       "             ('companies', 52),\n",
       "             ('sending', 26),\n",
       "             ('professionals', 26),\n",
       "             ('schools', 26),\n",
       "             ('educate', 52),\n",
       "             ('generation', 78),\n",
       "             ('guys', 26),\n",
       "             ('this', 104),\n",
       "             ('website', 26),\n",
       "             ('for', 52),\n",
       "             ('preambulatory', 26),\n",
       "             ('clause+operative', 26),\n",
       "             ('clause', 26),\n",
       "             ('headings', 26),\n",
       "             ('link', 26),\n",
       "             ('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "              26),\n",
       "             ('min', 26),\n",
       "             ('speech', 26),\n",
       "             ('should', 26),\n",
       "             ('passed', 26),\n",
       "             ('community', 52),\n",
       "             ('delegate', 26),\n",
       "             ('going', 26),\n",
       "             ('talk', 26),\n",
       "             ('first', 26),\n",
       "             ('delegates', 26),\n",
       "             ('’', 26),\n",
       "             ('supports', 26),\n",
       "             ('ban', 26),\n",
       "             ('which', 26),\n",
       "             ('helpful', 26),\n",
       "             ('international', 26),\n",
       "             ('creating', 26),\n",
       "             ('destructions', 26),\n",
       "             ('along', 26),\n",
       "             ('casualties', 26),\n",
       "             ('innocent', 26),\n",
       "             ('these', 52),\n",
       "             ('created', 26),\n",
       "             ('produce', 26),\n",
       "             ('solutions', 26),\n",
       "             ('base', 26),\n",
       "             ('on', 78),\n",
       "             ('vital', 26),\n",
       "             ('responsibility', 26),\n",
       "             ('dangers', 26),\n",
       "             ('harms', 26),\n",
       "             ('may', 52),\n",
       "             ('also', 78),\n",
       "             ('intentionally', 26),\n",
       "             ('used', 26),\n",
       "             ('different', 26),\n",
       "             ('focuses', 26),\n",
       "             ('include', 26),\n",
       "             ('current', 26),\n",
       "             ('harming', 52),\n",
       "             ('citizens', 26),\n",
       "             ('especially', 26),\n",
       "             ('assaulting', 26),\n",
       "             ('raping', 26),\n",
       "             ('our', 26),\n",
       "             ('further', 26),\n",
       "             ('effectively', 52),\n",
       "             ('block', 27),\n",
       "             ('giving', 26),\n",
       "             ('harsher', 94),\n",
       "             ('punishments', 26),\n",
       "             ('elongating', 26),\n",
       "             ('period', 26),\n",
       "             ('providing', 26),\n",
       "             ('criminals', 26),\n",
       "             ('but', 26),\n",
       "             ('most', 26),\n",
       "             ('eagerly', 26),\n",
       "             ('rebut', 26),\n",
       "             ('poi', 26),\n",
       "             ('since', 30),\n",
       "             ('will', 87),\n",
       "             ('learn', 52),\n",
       "             ('abt', 52),\n",
       "             ('at', 26),\n",
       "             ('age', 26),\n",
       "             ('they', 26),\n",
       "             ('capable', 26),\n",
       "             ('learning', 26),\n",
       "             ('more', 26),\n",
       "             ('able', 26),\n",
       "             ('parts', 26),\n",
       "             ('poi2', 26),\n",
       "             ('what', 26),\n",
       "             ('we', 50),\n",
       "             ('said', 26),\n",
       "             ('like', 26),\n",
       "             ('rules', 41),\n",
       "             ('illegal', 37),\n",
       "             ('take', 22),\n",
       "             ('care', 21),\n",
       "             ('poi3', 17),\n",
       "             ('sold', 8),\n",
       "             ('online', 7),\n",
       "             ('offline', 5)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size #unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[267,  82,  81, ..., 251, 250,  84],\n",
       "       [ 82,  81, 266, ..., 250,  84,  85],\n",
       "       [ 81, 266,   2, ...,  84,  85,   1],\n",
       "       ...,\n",
       "       [  9,  39,  24, ..., 265,  80,  40],\n",
       "       [ 39,  24,  77, ...,  80,  40,  26],\n",
       "       [ 24,  77,  73, ...,  40,  26,  83]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences #이렇게 문장이 한단어씩 추가되어 시쿼스 리스트로 입력된다. 이것은 id numbers로 각 단어를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[267,  82,  81, ...,  74, 251, 250],\n",
       "       [ 82,  81, 266, ..., 251, 250,  84],\n",
       "       [ 81, 266,   2, ..., 250,  84,  85],\n",
       "       ...,\n",
       "       [  9,  39,  24, ...,  16, 265,  80],\n",
       "       [ 39,  24,  77, ..., 265,  80,  40],\n",
       "       [ 24,  77,  73, ...,  80,  40,  26]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:,:-1] # 맨 마지막 단어를 하나 뺀 시퀀스를 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84,  85,   1,  86,  87,  82,  81,  88,  89,  90,  91,  92,  93,\n",
       "        94,   5,  95,  96,  97,   8,  98,  27,   2,  99, 100, 101,  10,\n",
       "       102,   6, 103, 104,  41,  42, 105, 106,  11,   7,   4, 107, 108,\n",
       "        12, 109, 110,   8, 111,   3,  43, 112,   2, 113,  44,  28,  12,\n",
       "       114, 115, 116,   1,  45, 117, 118,   5, 119,  44,  75,   3,  13,\n",
       "       120, 121,   6,   1,  45,   8,  46,  40, 122,   1, 123,  17, 124,\n",
       "        47,  42,  43, 125,   5, 126,  18,   3,   1, 127,  48,   2,  11,\n",
       "         4,  19, 128, 129,  25, 130, 131,   3, 132, 133,  49, 134,   7,\n",
       "         4,   3, 135, 136,  16, 137,  29,  30,   2,  50,   6,  19, 138,\n",
       "         1, 139,   2,  51, 140, 141,   7,   4, 142,  39,   8,  20, 143,\n",
       "        47, 144,  52,   3,  29,  28, 145, 146,   1, 147,  10,  53,   5,\n",
       "        31,  15, 148,  38,  12,  53, 149,  54, 150,  55, 151,  56,  16,\n",
       "       152,  57,  32,  31, 153,  58,  27,  59,  60,   7,   4,  61,  20,\n",
       "       154,  25, 155,   3, 156,   1,  62,   2,  11, 157,   6, 158, 159,\n",
       "         5, 160,   1,  14,  21,   1,  33, 161, 162,   6,   9,   6,  34,\n",
       "        19,  24,  13, 163, 164, 165, 166, 167,  16, 168, 169, 170, 171,\n",
       "         6,   1, 172,  34,  20, 173, 174,   1, 175,   2,   9,  49,  63,\n",
       "        34,  54, 176, 177,   3, 178,   3,  64,   1,  14,  35, 179,  62,\n",
       "        22, 180,  65, 181, 182, 183, 184, 185, 186,  19, 187, 188,  22,\n",
       "        18, 189,  15, 190,   6,   1,  66,  30,   2,   1,  50,   8,  22,\n",
       "       191,  10, 192,   3, 193,  21, 194,   1, 195, 196,  18, 197,   1,\n",
       "       198,   2,  13, 199,  10, 200,  65,   1, 201,  66,  30,  13,  17,\n",
       "       202,  33, 203, 204,  38,  46, 205,   2, 206,  29,  67,  13,  17,\n",
       "       207,   6,  63,   8,  33, 208,   9,  67, 209,  25, 210,  36,   1,\n",
       "       211, 212,   2,  40,   3,  64,   1,  14,  35,  21,   1, 213,   5,\n",
       "       214,   2,  11,   4,  11,   4,  68,  37, 215,  15, 216,  28, 217,\n",
       "       218,   5,  22,  68, 219,   7,   4,   7,   4,  37,  10,  12, 220,\n",
       "        48,  69, 221, 222,  32,   6, 223,   5, 224, 225,  18,  37, 226,\n",
       "        70,  83,   7,   4,   6, 227,  23, 228,   5, 229,   1,  51, 230,\n",
       "        16, 231,  52,   3,   7,   4, 232, 233, 234,  78,  55,  56,  57,\n",
       "        32,  31, 235,  58,  27,  59,  60,   7,   4,  61, 236,  20,   1,\n",
       "       237,  24,  80,   1,  14,  35,  26,  71,  72,   9, 238,  12,  14,\n",
       "       239, 240,  17, 241,   2, 242,  21,   9, 243,  70,   5,  26,  15,\n",
       "       244,   3,  71,  72,  69, 245,   5, 246,  23,  79,  36,   9,   8,\n",
       "        41, 247,  73, 248, 249,  23,  76,  36,   9,  39,  24,  77,  73,\n",
       "        26, 253, 255,   2,   8,  38, 259,  23,  76,  39,  24,  77,   9,\n",
       "        26,  15, 263, 264,  16, 265,  80,  40,  26,  83])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:,-1] #전체 문장에서 마지막 단어를 하나씩 추출해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1] #X 가 입력되면\n",
    "y = sequences[:,-1] #Y  가 출력되어 예측하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478, 25)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape #25 개의 단어로 구성된 문장이 487 개가 있다는 거지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size,seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size,seq_len,input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 25)            6700      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 25, 150)           105600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 268)               40468     \n",
      "=================================================================\n",
      "Total params: 356,018\n",
      "Trainable params: 356,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-6276a6d90660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\KERAS\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\KERAS\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         raise ImportError(\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "478/478 [==============================] - 0s 514us/step - loss: 3.0375 - accuracy: 0.2155\n",
      "Epoch 2/300\n",
      "478/478 [==============================] - 0s 546us/step - loss: 3.0033 - accuracy: 0.2259\n",
      "Epoch 3/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 2.9892 - accuracy: 0.2134\n",
      "Epoch 4/300\n",
      "478/478 [==============================] - 0s 540us/step - loss: 2.9900 - accuracy: 0.2406\n",
      "Epoch 5/300\n",
      "478/478 [==============================] - 0s 574us/step - loss: 2.9433 - accuracy: 0.2364\n",
      "Epoch 6/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 2.9199 - accuracy: 0.2427\n",
      "Epoch 7/300\n",
      "478/478 [==============================] - 0s 521us/step - loss: 2.8919 - accuracy: 0.2385\n",
      "Epoch 8/300\n",
      "478/478 [==============================] - 0s 544us/step - loss: 2.8716 - accuracy: 0.2510\n",
      "Epoch 9/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 2.8595 - accuracy: 0.2531\n",
      "Epoch 10/300\n",
      "478/478 [==============================] - 0s 505us/step - loss: 2.8456 - accuracy: 0.2720\n",
      "Epoch 11/300\n",
      "478/478 [==============================] - 0s 543us/step - loss: 2.8147 - accuracy: 0.2887\n",
      "Epoch 12/300\n",
      "478/478 [==============================] - 0s 513us/step - loss: 2.8118 - accuracy: 0.2762\n",
      "Epoch 13/300\n",
      "478/478 [==============================] - 0s 501us/step - loss: 2.7831 - accuracy: 0.2824\n",
      "Epoch 14/300\n",
      "478/478 [==============================] - 0s 498us/step - loss: 2.7824 - accuracy: 0.2657\n",
      "Epoch 15/300\n",
      "478/478 [==============================] - 0s 512us/step - loss: 2.7711 - accuracy: 0.2699\n",
      "Epoch 16/300\n",
      "478/478 [==============================] - 0s 530us/step - loss: 2.7703 - accuracy: 0.2803\n",
      "Epoch 17/300\n",
      "478/478 [==============================] - 0s 512us/step - loss: 2.7605 - accuracy: 0.2762\n",
      "Epoch 18/300\n",
      "478/478 [==============================] - 0s 565us/step - loss: 2.7746 - accuracy: 0.2782\n",
      "Epoch 19/300\n",
      "478/478 [==============================] - 0s 552us/step - loss: 2.8358 - accuracy: 0.2531\n",
      "Epoch 20/300\n",
      "478/478 [==============================] - 0s 517us/step - loss: 2.8104 - accuracy: 0.2741\n",
      "Epoch 21/300\n",
      "478/478 [==============================] - 0s 515us/step - loss: 2.7673 - accuracy: 0.2824\n",
      "Epoch 22/300\n",
      "478/478 [==============================] - 0s 497us/step - loss: 2.8313 - accuracy: 0.2741\n",
      "Epoch 23/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 2.8265 - accuracy: 0.2552\n",
      "Epoch 24/300\n",
      "478/478 [==============================] - 0s 516us/step - loss: 2.8211 - accuracy: 0.2636\n",
      "Epoch 25/300\n",
      "478/478 [==============================] - 0s 535us/step - loss: 2.7871 - accuracy: 0.2615\n",
      "Epoch 26/300\n",
      "478/478 [==============================] - 0s 489us/step - loss: 2.7688 - accuracy: 0.2573\n",
      "Epoch 27/300\n",
      "478/478 [==============================] - 0s 515us/step - loss: 2.7281 - accuracy: 0.2699\n",
      "Epoch 28/300\n",
      "478/478 [==============================] - 0s 510us/step - loss: 2.7215 - accuracy: 0.2929\n",
      "Epoch 29/300\n",
      "478/478 [==============================] - 0s 544us/step - loss: 2.7084 - accuracy: 0.2929\n",
      "Epoch 30/300\n",
      "478/478 [==============================] - 0s 484us/step - loss: 2.7108 - accuracy: 0.2657\n",
      "Epoch 31/300\n",
      "478/478 [==============================] - 0s 588us/step - loss: 2.7004 - accuracy: 0.2615\n",
      "Epoch 32/300\n",
      "478/478 [==============================] - 0s 567us/step - loss: 2.6428 - accuracy: 0.2866\n",
      "Epoch 33/300\n",
      "478/478 [==============================] - 0s 530us/step - loss: 2.6128 - accuracy: 0.3285\n",
      "Epoch 34/300\n",
      "478/478 [==============================] - 0s 509us/step - loss: 2.6590 - accuracy: 0.2824\n",
      "Epoch 35/300\n",
      "478/478 [==============================] - 0s 490us/step - loss: 2.6466 - accuracy: 0.2950\n",
      "Epoch 36/300\n",
      "478/478 [==============================] - 0s 526us/step - loss: 2.6553 - accuracy: 0.2762\n",
      "Epoch 37/300\n",
      "478/478 [==============================] - 0s 525us/step - loss: 2.5999 - accuracy: 0.3222\n",
      "Epoch 38/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 2.5851 - accuracy: 0.2971\n",
      "Epoch 39/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 2.5656 - accuracy: 0.3201\n",
      "Epoch 40/300\n",
      "478/478 [==============================] - 0s 531us/step - loss: 2.5604 - accuracy: 0.3075\n",
      "Epoch 41/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 2.5652 - accuracy: 0.3117\n",
      "Epoch 42/300\n",
      "478/478 [==============================] - 0s 536us/step - loss: 2.5522 - accuracy: 0.3347\n",
      "Epoch 43/300\n",
      "478/478 [==============================] - 0s 510us/step - loss: 2.5087 - accuracy: 0.3515\n",
      "Epoch 44/300\n",
      "478/478 [==============================] - 0s 543us/step - loss: 2.4931 - accuracy: 0.3410\n",
      "Epoch 45/300\n",
      "478/478 [==============================] - 0s 555us/step - loss: 2.4513 - accuracy: 0.3410\n",
      "Epoch 46/300\n",
      "478/478 [==============================] - 0s 509us/step - loss: 2.4387 - accuracy: 0.3556\n",
      "Epoch 47/300\n",
      "478/478 [==============================] - 0s 475us/step - loss: 2.4148 - accuracy: 0.3619\n",
      "Epoch 48/300\n",
      "478/478 [==============================] - 0s 511us/step - loss: 2.4072 - accuracy: 0.3787\n",
      "Epoch 49/300\n",
      "478/478 [==============================] - 0s 501us/step - loss: 2.4034 - accuracy: 0.3661\n",
      "Epoch 50/300\n",
      "478/478 [==============================] - 0s 564us/step - loss: 2.4020 - accuracy: 0.3682\n",
      "Epoch 51/300\n",
      "478/478 [==============================] - 0s 521us/step - loss: 2.3624 - accuracy: 0.3828\n",
      "Epoch 52/300\n",
      "478/478 [==============================] - 0s 507us/step - loss: 2.3469 - accuracy: 0.3912\n",
      "Epoch 53/300\n",
      "478/478 [==============================] - 0s 495us/step - loss: 2.3329 - accuracy: 0.3849\n",
      "Epoch 54/300\n",
      "478/478 [==============================] - 0s 520us/step - loss: 2.3123 - accuracy: 0.3996\n",
      "Epoch 55/300\n",
      "478/478 [==============================] - 0s 507us/step - loss: 2.3218 - accuracy: 0.3954\n",
      "Epoch 56/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 2.3385 - accuracy: 0.3828\n",
      "Epoch 57/300\n",
      "478/478 [==============================] - 0s 549us/step - loss: 2.3305 - accuracy: 0.3787\n",
      "Epoch 58/300\n",
      "478/478 [==============================] - 0s 639us/step - loss: 2.3296 - accuracy: 0.3745\n",
      "Epoch 59/300\n",
      "478/478 [==============================] - 0s 526us/step - loss: 2.3369 - accuracy: 0.3808\n",
      "Epoch 60/300\n",
      "478/478 [==============================] - 0s 476us/step - loss: 2.3082 - accuracy: 0.3766\n",
      "Epoch 61/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 2.3074 - accuracy: 0.3828\n",
      "Epoch 62/300\n",
      "478/478 [==============================] - 0s 500us/step - loss: 2.3245 - accuracy: 0.3703\n",
      "Epoch 63/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 2.3114 - accuracy: 0.3849\n",
      "Epoch 64/300\n",
      "478/478 [==============================] - 0s 475us/step - loss: 2.3227 - accuracy: 0.3766\n",
      "Epoch 65/300\n",
      "478/478 [==============================] - 0s 473us/step - loss: 2.3071 - accuracy: 0.3870\n",
      "Epoch 66/300\n",
      "478/478 [==============================] - 0s 455us/step - loss: 2.3107 - accuracy: 0.3891\n",
      "Epoch 67/300\n",
      "478/478 [==============================] - 0s 507us/step - loss: 2.2666 - accuracy: 0.4100\n",
      "Epoch 68/300\n",
      "478/478 [==============================] - 0s 499us/step - loss: 2.2606 - accuracy: 0.3891\n",
      "Epoch 69/300\n",
      "478/478 [==============================] - 0s 480us/step - loss: 2.2297 - accuracy: 0.3933\n",
      "Epoch 70/300\n",
      "478/478 [==============================] - 0s 478us/step - loss: 2.2053 - accuracy: 0.4100\n",
      "Epoch 71/300\n",
      "478/478 [==============================] - 0s 528us/step - loss: 2.1817 - accuracy: 0.4163\n",
      "Epoch 72/300\n",
      "478/478 [==============================] - 0s 603us/step - loss: 2.1714 - accuracy: 0.4079\n",
      "Epoch 73/300\n",
      "478/478 [==============================] - 0s 494us/step - loss: 2.1850 - accuracy: 0.3912\n",
      "Epoch 74/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 2.1631 - accuracy: 0.4121\n",
      "Epoch 75/300\n",
      "478/478 [==============================] - 0s 503us/step - loss: 2.2125 - accuracy: 0.3912\n",
      "Epoch 76/300\n",
      "478/478 [==============================] - 0s 495us/step - loss: 2.3753 - accuracy: 0.3410\n",
      "Epoch 77/300\n",
      "478/478 [==============================] - 0s 490us/step - loss: 2.2838 - accuracy: 0.3682\n",
      "Epoch 78/300\n",
      "478/478 [==============================] - 0s 485us/step - loss: 2.3607 - accuracy: 0.3494\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478/478 [==============================] - 0s 494us/step - loss: 2.3257 - accuracy: 0.3536\n",
      "Epoch 80/300\n",
      "478/478 [==============================] - 0s 484us/step - loss: 2.4333 - accuracy: 0.2929\n",
      "Epoch 81/300\n",
      "478/478 [==============================] - 0s 467us/step - loss: 2.4167 - accuracy: 0.3222\n",
      "Epoch 82/300\n",
      "478/478 [==============================] - 0s 472us/step - loss: 2.3095 - accuracy: 0.3745\n",
      "Epoch 83/300\n",
      "478/478 [==============================] - 0s 473us/step - loss: 2.3192 - accuracy: 0.3556\n",
      "Epoch 84/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 2.2030 - accuracy: 0.4038\n",
      "Epoch 85/300\n",
      "478/478 [==============================] - 0s 477us/step - loss: 2.1863 - accuracy: 0.3933\n",
      "Epoch 86/300\n",
      "478/478 [==============================] - 0s 549us/step - loss: 2.1834 - accuracy: 0.3870\n",
      "Epoch 87/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 2.1061 - accuracy: 0.4017\n",
      "Epoch 88/300\n",
      "478/478 [==============================] - 0s 494us/step - loss: 2.0877 - accuracy: 0.4393\n",
      "Epoch 89/300\n",
      "478/478 [==============================] - 0s 495us/step - loss: 2.0672 - accuracy: 0.4393\n",
      "Epoch 90/300\n",
      "478/478 [==============================] - 0s 451us/step - loss: 2.0289 - accuracy: 0.4456\n",
      "Epoch 91/300\n",
      "478/478 [==============================] - 0s 467us/step - loss: 2.0193 - accuracy: 0.4540\n",
      "Epoch 92/300\n",
      "478/478 [==============================] - 0s 467us/step - loss: 1.9819 - accuracy: 0.4749\n",
      "Epoch 93/300\n",
      "478/478 [==============================] - 0s 510us/step - loss: 1.9618 - accuracy: 0.4707\n",
      "Epoch 94/300\n",
      "478/478 [==============================] - 0s 473us/step - loss: 2.0015 - accuracy: 0.4686\n",
      "Epoch 95/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 2.0250 - accuracy: 0.4498\n",
      "Epoch 96/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 2.0252 - accuracy: 0.4665\n",
      "Epoch 97/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 1.9892 - accuracy: 0.4749\n",
      "Epoch 98/300\n",
      "478/478 [==============================] - 0s 465us/step - loss: 1.9566 - accuracy: 0.4854\n",
      "Epoch 99/300\n",
      "478/478 [==============================] - 0s 485us/step - loss: 1.9238 - accuracy: 0.4728\n",
      "Epoch 100/300\n",
      "478/478 [==============================] - 0s 600us/step - loss: 1.9587 - accuracy: 0.4707\n",
      "Epoch 101/300\n",
      "478/478 [==============================] - 0s 511us/step - loss: 1.9215 - accuracy: 0.4895\n",
      "Epoch 102/300\n",
      "478/478 [==============================] - 0s 485us/step - loss: 1.8972 - accuracy: 0.4854\n",
      "Epoch 103/300\n",
      "478/478 [==============================] - 0s 482us/step - loss: 1.8752 - accuracy: 0.5126\n",
      "Epoch 104/300\n",
      "478/478 [==============================] - 0s 535us/step - loss: 1.8388 - accuracy: 0.5021\n",
      "Epoch 105/300\n",
      "478/478 [==============================] - 0s 493us/step - loss: 1.8640 - accuracy: 0.4874\n",
      "Epoch 106/300\n",
      "478/478 [==============================] - 0s 485us/step - loss: 1.8441 - accuracy: 0.5021\n",
      "Epoch 107/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 1.8204 - accuracy: 0.5042\n",
      "Epoch 108/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 1.8272 - accuracy: 0.4958\n",
      "Epoch 109/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 1.8107 - accuracy: 0.5000\n",
      "Epoch 110/300\n",
      "478/478 [==============================] - 0s 480us/step - loss: 1.7954 - accuracy: 0.5126\n",
      "Epoch 111/300\n",
      "478/478 [==============================] - 0s 472us/step - loss: 1.8233 - accuracy: 0.4874\n",
      "Epoch 112/300\n",
      "478/478 [==============================] - 0s 468us/step - loss: 1.7991 - accuracy: 0.4937\n",
      "Epoch 113/300\n",
      "478/478 [==============================] - 0s 471us/step - loss: 1.7630 - accuracy: 0.5105\n",
      "Epoch 114/300\n",
      "478/478 [==============================] - 0s 641us/step - loss: 1.7272 - accuracy: 0.5251\n",
      "Epoch 115/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 1.6955 - accuracy: 0.5418\n",
      "Epoch 116/300\n",
      "478/478 [==============================] - 0s 531us/step - loss: 1.6794 - accuracy: 0.5439\n",
      "Epoch 117/300\n",
      "478/478 [==============================] - 0s 509us/step - loss: 1.6669 - accuracy: 0.5356\n",
      "Epoch 118/300\n",
      "478/478 [==============================] - 0s 476us/step - loss: 1.6440 - accuracy: 0.5502\n",
      "Epoch 119/300\n",
      "478/478 [==============================] - 0s 508us/step - loss: 1.6301 - accuracy: 0.5795\n",
      "Epoch 120/300\n",
      "478/478 [==============================] - 0s 499us/step - loss: 1.6994 - accuracy: 0.5397\n",
      "Epoch 121/300\n",
      "478/478 [==============================] - 0s 515us/step - loss: 1.6588 - accuracy: 0.5356\n",
      "Epoch 122/300\n",
      "478/478 [==============================] - 0s 508us/step - loss: 1.6782 - accuracy: 0.5356\n",
      "Epoch 123/300\n",
      "478/478 [==============================] - 0s 501us/step - loss: 1.6920 - accuracy: 0.5188\n",
      "Epoch 124/300\n",
      "478/478 [==============================] - 0s 514us/step - loss: 1.7075 - accuracy: 0.5042\n",
      "Epoch 125/300\n",
      "478/478 [==============================] - 0s 529us/step - loss: 1.7323 - accuracy: 0.4979\n",
      "Epoch 126/300\n",
      "478/478 [==============================] - 0s 503us/step - loss: 1.7508 - accuracy: 0.4812\n",
      "Epoch 127/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 1.6679 - accuracy: 0.5377\n",
      "Epoch 128/300\n",
      "478/478 [==============================] - 0s 584us/step - loss: 1.6864 - accuracy: 0.5230\n",
      "Epoch 129/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 1.8043 - accuracy: 0.4791\n",
      "Epoch 130/300\n",
      "478/478 [==============================] - 0s 481us/step - loss: 1.7641 - accuracy: 0.4958\n",
      "Epoch 131/300\n",
      "478/478 [==============================] - 0s 526us/step - loss: 1.7075 - accuracy: 0.5084\n",
      "Epoch 132/300\n",
      "478/478 [==============================] - 0s 504us/step - loss: 1.7058 - accuracy: 0.4854\n",
      "Epoch 133/300\n",
      "478/478 [==============================] - 0s 492us/step - loss: 1.6375 - accuracy: 0.5230\n",
      "Epoch 134/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 1.6019 - accuracy: 0.5481\n",
      "Epoch 135/300\n",
      "478/478 [==============================] - 0s 504us/step - loss: 1.5704 - accuracy: 0.5460\n",
      "Epoch 136/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 1.5327 - accuracy: 0.5649\n",
      "Epoch 137/300\n",
      "478/478 [==============================] - 0s 490us/step - loss: 1.5288 - accuracy: 0.5753\n",
      "Epoch 138/300\n",
      "478/478 [==============================] - 0s 471us/step - loss: 1.5028 - accuracy: 0.5774\n",
      "Epoch 139/300\n",
      "478/478 [==============================] - 0s 469us/step - loss: 1.4863 - accuracy: 0.5774\n",
      "Epoch 140/300\n",
      "478/478 [==============================] - 0s 464us/step - loss: 1.4705 - accuracy: 0.5816\n",
      "Epoch 141/300\n",
      "478/478 [==============================] - 0s 542us/step - loss: 1.4504 - accuracy: 0.5921\n",
      "Epoch 142/300\n",
      "478/478 [==============================] - 0s 533us/step - loss: 1.4329 - accuracy: 0.5921\n",
      "Epoch 143/300\n",
      "478/478 [==============================] - 0s 476us/step - loss: 1.4368 - accuracy: 0.5900\n",
      "Epoch 144/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 1.4156 - accuracy: 0.6109\n",
      "Epoch 145/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 1.3920 - accuracy: 0.6067\n",
      "Epoch 146/300\n",
      "478/478 [==============================] - 0s 484us/step - loss: 1.3910 - accuracy: 0.6067\n",
      "Epoch 147/300\n",
      "478/478 [==============================] - 0s 478us/step - loss: 1.3750 - accuracy: 0.5941\n",
      "Epoch 148/300\n",
      "478/478 [==============================] - 0s 477us/step - loss: 1.3586 - accuracy: 0.6297\n",
      "Epoch 149/300\n",
      "478/478 [==============================] - 0s 495us/step - loss: 1.3654 - accuracy: 0.6130\n",
      "Epoch 150/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 1.3591 - accuracy: 0.6088\n",
      "Epoch 151/300\n",
      "478/478 [==============================] - 0s 493us/step - loss: 1.3870 - accuracy: 0.5962\n",
      "Epoch 152/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 1.5030 - accuracy: 0.5628\n",
      "Epoch 153/300\n",
      "478/478 [==============================] - 0s 482us/step - loss: 1.5995 - accuracy: 0.5481\n",
      "Epoch 154/300\n",
      "478/478 [==============================] - 0s 476us/step - loss: 1.5706 - accuracy: 0.5314\n",
      "Epoch 155/300\n",
      "478/478 [==============================] - 0s 512us/step - loss: 1.5841 - accuracy: 0.5167\n",
      "Epoch 156/300\n",
      "478/478 [==============================] - 0s 567us/step - loss: 1.5499 - accuracy: 0.5356\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478/478 [==============================] - 0s 495us/step - loss: 1.5259 - accuracy: 0.5335\n",
      "Epoch 158/300\n",
      "478/478 [==============================] - 0s 457us/step - loss: 1.5678 - accuracy: 0.5063\n",
      "Epoch 159/300\n",
      "478/478 [==============================] - 0s 500us/step - loss: 1.6012 - accuracy: 0.4791\n",
      "Epoch 160/300\n",
      "478/478 [==============================] - 0s 468us/step - loss: 1.5423 - accuracy: 0.5146\n",
      "Epoch 161/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 1.5041 - accuracy: 0.5314\n",
      "Epoch 162/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 1.5819 - accuracy: 0.5042\n",
      "Epoch 163/300\n",
      "478/478 [==============================] - 0s 489us/step - loss: 1.5515 - accuracy: 0.4895\n",
      "Epoch 164/300\n",
      "478/478 [==============================] - 0s 477us/step - loss: 1.6763 - accuracy: 0.4854\n",
      "Epoch 165/300\n",
      "478/478 [==============================] - 0s 468us/step - loss: 1.6187 - accuracy: 0.5084\n",
      "Epoch 166/300\n",
      "478/478 [==============================] - 0s 471us/step - loss: 1.5679 - accuracy: 0.4916\n",
      "Epoch 167/300\n",
      "478/478 [==============================] - 0s 500us/step - loss: 1.6824 - accuracy: 0.4707\n",
      "Epoch 168/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 1.5029 - accuracy: 0.5356\n",
      "Epoch 169/300\n",
      "478/478 [==============================] - 0s 552us/step - loss: 1.4761 - accuracy: 0.5356\n",
      "Epoch 170/300\n",
      "478/478 [==============================] - 0s 534us/step - loss: 1.4894 - accuracy: 0.5502\n",
      "Epoch 171/300\n",
      "478/478 [==============================] - 0s 494us/step - loss: 1.4426 - accuracy: 0.5753\n",
      "Epoch 172/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 1.4631 - accuracy: 0.5502\n",
      "Epoch 173/300\n",
      "478/478 [==============================] - 0s 475us/step - loss: 1.5568 - accuracy: 0.5356\n",
      "Epoch 174/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 1.5552 - accuracy: 0.5272\n",
      "Epoch 175/300\n",
      "478/478 [==============================] - 0s 468us/step - loss: 1.5024 - accuracy: 0.5167\n",
      "Epoch 176/300\n",
      "478/478 [==============================] - 0s 482us/step - loss: 1.4219 - accuracy: 0.5607\n",
      "Epoch 177/300\n",
      "478/478 [==============================] - 0s 482us/step - loss: 1.4453 - accuracy: 0.5774\n",
      "Epoch 178/300\n",
      "478/478 [==============================] - 0s 469us/step - loss: 1.3837 - accuracy: 0.5753\n",
      "Epoch 179/300\n",
      "478/478 [==============================] - 0s 462us/step - loss: 1.3674 - accuracy: 0.5816\n",
      "Epoch 180/300\n",
      "478/478 [==============================] - 0s 484us/step - loss: 1.2868 - accuracy: 0.5941\n",
      "Epoch 181/300\n",
      "478/478 [==============================] - 0s 467us/step - loss: 1.2500 - accuracy: 0.6297\n",
      "Epoch 182/300\n",
      "478/478 [==============================] - 0s 496us/step - loss: 1.2063 - accuracy: 0.6423\n",
      "Epoch 183/300\n",
      "478/478 [==============================] - 0s 532us/step - loss: 1.1716 - accuracy: 0.6381\n",
      "Epoch 184/300\n",
      "478/478 [==============================] - 0s 670us/step - loss: 1.1739 - accuracy: 0.6527\n",
      "Epoch 185/300\n",
      "478/478 [==============================] - 0s 529us/step - loss: 1.1529 - accuracy: 0.6841\n",
      "Epoch 186/300\n",
      "478/478 [==============================] - 0s 512us/step - loss: 1.1249 - accuracy: 0.6674\n",
      "Epoch 187/300\n",
      "478/478 [==============================] - 0s 557us/step - loss: 1.1008 - accuracy: 0.6757\n",
      "Epoch 188/300\n",
      "478/478 [==============================] - 0s 543us/step - loss: 1.0812 - accuracy: 0.6904\n",
      "Epoch 189/300\n",
      "478/478 [==============================] - 0s 545us/step - loss: 1.0850 - accuracy: 0.6883\n",
      "Epoch 190/300\n",
      "478/478 [==============================] - 0s 499us/step - loss: 1.0885 - accuracy: 0.6820\n",
      "Epoch 191/300\n",
      "478/478 [==============================] - 0s 534us/step - loss: 1.1068 - accuracy: 0.6715\n",
      "Epoch 192/300\n",
      "478/478 [==============================] - 0s 528us/step - loss: 1.1007 - accuracy: 0.6715\n",
      "Epoch 193/300\n",
      "478/478 [==============================] - 0s 517us/step - loss: 1.0601 - accuracy: 0.6925\n",
      "Epoch 194/300\n",
      "478/478 [==============================] - 0s 521us/step - loss: 1.0550 - accuracy: 0.6967\n",
      "Epoch 195/300\n",
      "478/478 [==============================] - 0s 599us/step - loss: 1.0479 - accuracy: 0.6883\n",
      "Epoch 196/300\n",
      "478/478 [==============================] - 0s 574us/step - loss: 1.0239 - accuracy: 0.7113\n",
      "Epoch 197/300\n",
      "478/478 [==============================] - 0s 593us/step - loss: 0.9991 - accuracy: 0.7092\n",
      "Epoch 198/300\n",
      "478/478 [==============================] - 0s 874us/step - loss: 0.9856 - accuracy: 0.7197\n",
      "Epoch 199/300\n",
      "478/478 [==============================] - 0s 566us/step - loss: 0.9682 - accuracy: 0.7176\n",
      "Epoch 200/300\n",
      "478/478 [==============================] - 0s 534us/step - loss: 0.9828 - accuracy: 0.7238\n",
      "Epoch 201/300\n",
      "478/478 [==============================] - 0s 501us/step - loss: 0.9683 - accuracy: 0.7259\n",
      "Epoch 202/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 0.9656 - accuracy: 0.7238\n",
      "Epoch 203/300\n",
      "478/478 [==============================] - 0s 523us/step - loss: 0.9509 - accuracy: 0.7469\n",
      "Epoch 204/300\n",
      "478/478 [==============================] - 0s 499us/step - loss: 0.9529 - accuracy: 0.7134\n",
      "Epoch 205/300\n",
      "478/478 [==============================] - 0s 466us/step - loss: 0.9509 - accuracy: 0.7218\n",
      "Epoch 206/300\n",
      "478/478 [==============================] - 0s 492us/step - loss: 0.9450 - accuracy: 0.7238\n",
      "Epoch 207/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 0.9197 - accuracy: 0.7343\n",
      "Epoch 208/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 0.9183 - accuracy: 0.7364\n",
      "Epoch 209/300\n",
      "478/478 [==============================] - 0s 481us/step - loss: 0.9099 - accuracy: 0.7343\n",
      "Epoch 210/300\n",
      "478/478 [==============================] - 0s 511us/step - loss: 0.8901 - accuracy: 0.7448\n",
      "Epoch 211/300\n",
      "478/478 [==============================] - 0s 544us/step - loss: 0.8917 - accuracy: 0.7427\n",
      "Epoch 212/300\n",
      "478/478 [==============================] - 0s 498us/step - loss: 0.8695 - accuracy: 0.7510\n",
      "Epoch 213/300\n",
      "478/478 [==============================] - 0s 505us/step - loss: 0.8577 - accuracy: 0.7636\n",
      "Epoch 214/300\n",
      "478/478 [==============================] - 0s 506us/step - loss: 0.8453 - accuracy: 0.7531\n",
      "Epoch 215/300\n",
      "478/478 [==============================] - 0s 520us/step - loss: 0.8345 - accuracy: 0.7803\n",
      "Epoch 216/300\n",
      "478/478 [==============================] - 0s 516us/step - loss: 0.8266 - accuracy: 0.7782\n",
      "Epoch 217/300\n",
      "478/478 [==============================] - 0s 481us/step - loss: 0.8203 - accuracy: 0.7720\n",
      "Epoch 218/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 0.8187 - accuracy: 0.7762\n",
      "Epoch 219/300\n",
      "478/478 [==============================] - 0s 513us/step - loss: 0.8051 - accuracy: 0.7803\n",
      "Epoch 220/300\n",
      "478/478 [==============================] - 0s 499us/step - loss: 0.8086 - accuracy: 0.76780s - loss: 0.7896 - accuracy: 0.\n",
      "Epoch 221/300\n",
      "478/478 [==============================] - 0s 499us/step - loss: 0.7977 - accuracy: 0.7803\n",
      "Epoch 222/300\n",
      "478/478 [==============================] - 0s 481us/step - loss: 0.7902 - accuracy: 0.7866\n",
      "Epoch 223/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 0.8091 - accuracy: 0.7824\n",
      "Epoch 224/300\n",
      "478/478 [==============================] - 0s 496us/step - loss: 0.8274 - accuracy: 0.7531\n",
      "Epoch 225/300\n",
      "478/478 [==============================] - 0s 549us/step - loss: 0.8399 - accuracy: 0.7510\n",
      "Epoch 226/300\n",
      "478/478 [==============================] - 0s 503us/step - loss: 0.8544 - accuracy: 0.7427\n",
      "Epoch 227/300\n",
      "478/478 [==============================] - 0s 498us/step - loss: 0.8548 - accuracy: 0.7594\n",
      "Epoch 228/300\n",
      "478/478 [==============================] - 0s 473us/step - loss: 0.8665 - accuracy: 0.7280\n",
      "Epoch 229/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 0.8952 - accuracy: 0.7238\n",
      "Epoch 230/300\n",
      "478/478 [==============================] - 0s 531us/step - loss: 0.9009 - accuracy: 0.7092\n",
      "Epoch 231/300\n",
      "478/478 [==============================] - 0s 519us/step - loss: 0.8873 - accuracy: 0.7218\n",
      "Epoch 232/300\n",
      "478/478 [==============================] - 0s 489us/step - loss: 0.8682 - accuracy: 0.7343\n",
      "Epoch 233/300\n",
      "478/478 [==============================] - 0s 511us/step - loss: 0.8998 - accuracy: 0.7155\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478/478 [==============================] - 0s 490us/step - loss: 0.8432 - accuracy: 0.7322\n",
      "Epoch 235/300\n",
      "478/478 [==============================] - 0s 511us/step - loss: 0.8957 - accuracy: 0.7197\n",
      "Epoch 236/300\n",
      "478/478 [==============================] - 0s 537us/step - loss: 0.8844 - accuracy: 0.7197\n",
      "Epoch 237/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 0.9157 - accuracy: 0.7155\n",
      "Epoch 238/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 0.9754 - accuracy: 0.6862\n",
      "Epoch 239/300\n",
      "478/478 [==============================] - 0s 555us/step - loss: 0.9355 - accuracy: 0.6967\n",
      "Epoch 240/300\n",
      "478/478 [==============================] - 0s 567us/step - loss: 0.8713 - accuracy: 0.7050\n",
      "Epoch 241/300\n",
      "478/478 [==============================] - 0s 498us/step - loss: 0.8458 - accuracy: 0.7510\n",
      "Epoch 242/300\n",
      "478/478 [==============================] - 0s 552us/step - loss: 0.8361 - accuracy: 0.7448\n",
      "Epoch 243/300\n",
      "478/478 [==============================] - 0s 489us/step - loss: 0.8117 - accuracy: 0.7552\n",
      "Epoch 244/300\n",
      "478/478 [==============================] - 0s 471us/step - loss: 0.7943 - accuracy: 0.7510\n",
      "Epoch 245/300\n",
      "478/478 [==============================] - 0s 506us/step - loss: 0.7838 - accuracy: 0.7552\n",
      "Epoch 246/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 0.7398 - accuracy: 0.7720\n",
      "Epoch 247/300\n",
      "478/478 [==============================] - 0s 524us/step - loss: 0.7368 - accuracy: 0.7741\n",
      "Epoch 248/300\n",
      "478/478 [==============================] - 0s 515us/step - loss: 0.7313 - accuracy: 0.8117\n",
      "Epoch 249/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 0.7127 - accuracy: 0.8013\n",
      "Epoch 250/300\n",
      "478/478 [==============================] - 0s 510us/step - loss: 0.7091 - accuracy: 0.8054\n",
      "Epoch 251/300\n",
      "478/478 [==============================] - 0s 498us/step - loss: 0.7013 - accuracy: 0.7971\n",
      "Epoch 252/300\n",
      "478/478 [==============================] - 0s 515us/step - loss: 0.6774 - accuracy: 0.8243\n",
      "Epoch 253/300\n",
      "478/478 [==============================] - 0s 548us/step - loss: 0.6594 - accuracy: 0.8201\n",
      "Epoch 254/300\n",
      "478/478 [==============================] - 0s 556us/step - loss: 0.6622 - accuracy: 0.8117\n",
      "Epoch 255/300\n",
      "478/478 [==============================] - 0s 511us/step - loss: 0.6683 - accuracy: 0.8075\n",
      "Epoch 256/300\n",
      "478/478 [==============================] - 0s 514us/step - loss: 0.6437 - accuracy: 0.8201\n",
      "Epoch 257/300\n",
      "478/478 [==============================] - 0s 501us/step - loss: 0.6575 - accuracy: 0.8180\n",
      "Epoch 258/300\n",
      "478/478 [==============================] - 0s 492us/step - loss: 0.6660 - accuracy: 0.8054\n",
      "Epoch 259/300\n",
      "478/478 [==============================] - 0s 479us/step - loss: 0.7110 - accuracy: 0.8075\n",
      "Epoch 260/300\n",
      "478/478 [==============================] - 0s 494us/step - loss: 0.7533 - accuracy: 0.7699\n",
      "Epoch 261/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 0.7068 - accuracy: 0.7866\n",
      "Epoch 262/300\n",
      "478/478 [==============================] - 0s 481us/step - loss: 0.7124 - accuracy: 0.7720\n",
      "Epoch 263/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 0.7958 - accuracy: 0.7490\n",
      "Epoch 264/300\n",
      "478/478 [==============================] - 0s 482us/step - loss: 0.8654 - accuracy: 0.7176\n",
      "Epoch 265/300\n",
      "478/478 [==============================] - 0s 461us/step - loss: 0.8650 - accuracy: 0.7238\n",
      "Epoch 266/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 1.1448 - accuracy: 0.6653\n",
      "Epoch 267/300\n",
      "478/478 [==============================] - 0s 513us/step - loss: 1.1381 - accuracy: 0.6423\n",
      "Epoch 268/300\n",
      "478/478 [==============================] - 0s 549us/step - loss: 1.0665 - accuracy: 0.6569\n",
      "Epoch 269/300\n",
      "478/478 [==============================] - 0s 518us/step - loss: 1.0500 - accuracy: 0.6653\n",
      "Epoch 270/300\n",
      "478/478 [==============================] - 0s 472us/step - loss: 1.0825 - accuracy: 0.6736\n",
      "Epoch 271/300\n",
      "478/478 [==============================] - 0s 496us/step - loss: 1.1077 - accuracy: 0.6485\n",
      "Epoch 272/300\n",
      "478/478 [==============================] - 0s 485us/step - loss: 1.0101 - accuracy: 0.6674\n",
      "Epoch 273/300\n",
      "478/478 [==============================] - 0s 469us/step - loss: 1.0833 - accuracy: 0.6402\n",
      "Epoch 274/300\n",
      "478/478 [==============================] - 0s 473us/step - loss: 0.9198 - accuracy: 0.6904\n",
      "Epoch 275/300\n",
      "478/478 [==============================] - 0s 468us/step - loss: 0.9022 - accuracy: 0.7259\n",
      "Epoch 276/300\n",
      "478/478 [==============================] - 0s 470us/step - loss: 0.9647 - accuracy: 0.6904\n",
      "Epoch 277/300\n",
      "478/478 [==============================] - 0s 485us/step - loss: 0.8380 - accuracy: 0.7238\n",
      "Epoch 278/300\n",
      "478/478 [==============================] - 0s 520us/step - loss: 0.9158 - accuracy: 0.6841\n",
      "Epoch 279/300\n",
      "478/478 [==============================] - 0s 481us/step - loss: 0.8247 - accuracy: 0.7385\n",
      "Epoch 280/300\n",
      "478/478 [==============================] - 0s 477us/step - loss: 0.7494 - accuracy: 0.7741\n",
      "Epoch 281/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 0.7786 - accuracy: 0.7615\n",
      "Epoch 282/300\n",
      "478/478 [==============================] - 0s 566us/step - loss: 0.7082 - accuracy: 0.8054\n",
      "Epoch 283/300\n",
      "478/478 [==============================] - 0s 537us/step - loss: 0.6570 - accuracy: 0.8159\n",
      "Epoch 284/300\n",
      "478/478 [==============================] - 0s 494us/step - loss: 0.6380 - accuracy: 0.8264\n",
      "Epoch 285/300\n",
      "478/478 [==============================] - 0s 487us/step - loss: 0.6071 - accuracy: 0.8452\n",
      "Epoch 286/300\n",
      "478/478 [==============================] - 0s 491us/step - loss: 0.5858 - accuracy: 0.8473\n",
      "Epoch 287/300\n",
      "478/478 [==============================] - 0s 486us/step - loss: 0.6557 - accuracy: 0.8033\n",
      "Epoch 288/300\n",
      "478/478 [==============================] - 0s 488us/step - loss: 0.6256 - accuracy: 0.8243\n",
      "Epoch 289/300\n",
      "478/478 [==============================] - 0s 478us/step - loss: 0.5920 - accuracy: 0.8410\n",
      "Epoch 290/300\n",
      "478/478 [==============================] - 0s 478us/step - loss: 0.5709 - accuracy: 0.8494\n",
      "Epoch 291/300\n",
      "478/478 [==============================] - 0s 483us/step - loss: 0.5639 - accuracy: 0.8410\n",
      "Epoch 292/300\n",
      "478/478 [==============================] - 0s 471us/step - loss: 0.5716 - accuracy: 0.8640\n",
      "Epoch 293/300\n",
      "478/478 [==============================] - 0s 489us/step - loss: 0.5512 - accuracy: 0.8598\n",
      "Epoch 294/300\n",
      "478/478 [==============================] - 0s 467us/step - loss: 0.5795 - accuracy: 0.8473\n",
      "Epoch 295/300\n",
      "478/478 [==============================] - 0s 465us/step - loss: 0.5461 - accuracy: 0.8640\n",
      "Epoch 296/300\n",
      "478/478 [==============================] - 0s 484us/step - loss: 0.5153 - accuracy: 0.8724\n",
      "Epoch 297/300\n",
      "478/478 [==============================] - 0s 569us/step - loss: 0.5281 - accuracy: 0.8703\n",
      "Epoch 298/300\n",
      "478/478 [==============================] - 0s 517us/step - loss: 0.5324 - accuracy: 0.8619\n",
      "Epoch 299/300\n",
      "478/478 [==============================] - 0s 530us/step - loss: 0.5334 - accuracy: 0.8556\n",
      "Epoch 300/300\n",
      "478/478 [==============================] - 0s 482us/step - loss: 0.5023 - accuracy: 0.8745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c5d1269488>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=300,verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mun_writer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer, open('mun_tokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,seq_len,seed_text,num_gen_words):\n",
    "    \n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen = seq_len, truncating='pre')\n",
    "        \n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        \n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "    \n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['committee',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'question',\n",
       " 'of',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'significantly',\n",
       " 'reduce',\n",
       " 'all',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'related',\n",
       " 'death',\n",
       " 'rates',\n",
       " 'everywheremain',\n",
       " 'main',\n",
       " 'submitter',\n",
       " 'brazil',\n",
       " 'co',\n",
       " 'submitter',\n",
       " 'argentina',\n",
       " 'china',\n",
       " 'czech']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'ban',\n",
       " 'of',\n",
       " 'firearms',\n",
       " 'which',\n",
       " 'is',\n",
       " 'helpful',\n",
       " 'for',\n",
       " 'the',\n",
       " 'international',\n",
       " 'community',\n",
       " 'because',\n",
       " 'firearms',\n",
       " 'are',\n",
       " 'creating',\n",
       " 'mass',\n",
       " 'destructions',\n",
       " 'along',\n",
       " 'with',\n",
       " 'numerous',\n",
       " 'casualties',\n",
       " 'of',\n",
       " 'innocent',\n",
       " 'people',\n",
       " 'these',\n",
       " 'firearms']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the ban of firearms which is helpful for the international community because firearms are creating mass destructions along with numerous casualties of innocent people these firearms'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are created by companies that mass produce guns these solutions all base on the vital responsibility of countries to educate the young generation about the'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25) #문장이 생성된다. 결의문 작성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
